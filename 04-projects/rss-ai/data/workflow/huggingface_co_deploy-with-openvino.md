# Optimize and deploy models with Optimum-Intel and OpenVINO GenAI

**Source**: HuggingFace
**Date**: time.struct_time(tm_year=2024, tm_mon=9, tm_mday=20, tm_hour=0, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=264, tm_isdst=0)

- OpenVINO™ is recommended for deploying large language models (LLMs) at the edge and client-side due to its C++ AI inference solution and the GenAI API for integrating LLMs into C++ or Python applications.
- The process involves setting up the environment, exporting models to OpenVINO Intermediate Representation (IR), optimizing the model through techniques like weight-only quantization, and deploying the model with OpenVINO GenAI API which provides Python and C++ APIs.
- By following these steps using Optimum-Intel and OpenVINO™ GenAI, users can achieve optimized, high-performance AI inference in environments where Python may not be ideal, ensuring smooth operation on Intel hardware.

[Read more](https://huggingface.co/blog/deploy-with-openvino)
