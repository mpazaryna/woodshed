# Llama can now see and run on your device - welcome Llama 3.2

**Source**: HuggingFace
**Date**: time.struct_time(tm_year=2024, tm_mon=9, tm_mday=25, tm_hour=0, tm_min=0, tm_sec=0, tm_wday=2, tm_yday=269, tm_isdst=0)

- Llama 3.2 Vision is a powerful multimodal model that excels in tasks like visual understanding, document question answering, and image-text retrieval. It supports multiple languages and can handle text-only or image-text input.
- Llama 3.2 includes 1B and 3B text models designed for on-device use, such as in multilingual knowledge retrieval and prompt rewriting. These models perform well and compete with larger models.
- Llama 3.2 models can be run on devices using tools like llama.cpp, Transformers.js, and MLC.ai Web-LLM, allowing for efficient inference and use cases such as fine-tuning and chat completions.

[Read more](https://huggingface.co/blog/llama32)
