{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd7c4c87",
   "metadata": {},
   "source": [
    "# Router Engine\n",
    "\n",
    "Given a query, the router will pick what to run.  This shows dynamic query understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357c97c9",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab5f4f4a-5890-451c-8869-24606ef9f396",
   "metadata": {
    "height": 64,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this helper method is not required when running the code on lightning.ai\n",
    "# it is only required when running the code locally\n",
    "from helper import get_openai_api_key\n",
    "OPENAI_API_KEY = get_openai_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffc9b4f4-64d4-4266-9889-54db90e00ee9",
   "metadata": {
    "height": 64,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this is required since Juypyter notebook is not async by default.\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fca250",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c7f012d-dcd3-4881-a568-72dd27d79159",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# load documents\n",
    "documents = SimpleDirectoryReader(input_files=[\"/teamspace/uploads/metagpt.pdf\"]).load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b48a301",
   "metadata": {},
   "source": [
    "## Split document into nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a537bc0-78ee-4dda-a43f-60fd80062df6",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61afc8bf",
   "metadata": {},
   "source": [
    "## Define LLM and Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de0660ee-b231-4351-b158-d8ad023e00b5",
   "metadata": {
    "height": 115,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997c7559",
   "metadata": {},
   "source": [
    "## Define Indexes\n",
    "\n",
    "- Summary Index \n",
    "- Vector Index \n",
    "\n",
    "Think of the index as a set of metadata over the data.  Different indexes have different index behavior.  A vector index indexes node through text embeddings.  Querying a vector index in llamaindex will return the most similar nodes. A summary index is also a simple index, and a query to it will return all of the nodes that are currently in the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73d01b01-bc74-432a-8d92-07b9e86498b0",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core import SummaryIndex, VectorStoreIndex\n",
    "\n",
    "summary_index = SummaryIndex(nodes)\n",
    "vector_index = VectorStoreIndex(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9898d3f",
   "metadata": {},
   "source": [
    "## Turn the indexes into query engines\n",
    "\n",
    "Each query engine represents an interface over the data that has been stored in the index.  It combines retrieval with LLM synthesis.  Each engine is specific for a certain kind of question.  This is where the use case of the router becomes significant.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44cd7046-c714-4920-b077-b3ded917862f",
   "metadata": {
    "height": 98,
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary_query_engine = summary_index.as_query_engine(\n",
    "    response_mode=\"tree_summarize\",\n",
    "    use_async=True,\n",
    ")\n",
    "\n",
    "vector_query_engine = vector_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae6d44d",
   "metadata": {},
   "source": [
    "## Create query tools\n",
    "\n",
    "The query tools are simply the query engine with additional metadata.  The metadata reflects the kind of information the tool can answer.  For example, asking it to \"summarize\" the article would launch the summary_tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a1d6d75-247e-426a-8ef4-b49225c24796",
   "metadata": {
    "height": 285,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "summary_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=summary_query_engine,\n",
    "    description=(\n",
    "        \"Useful for summarization questions related to MetaGPT\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "vector_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=vector_query_engine,\n",
    "    description=(\n",
    "        \"Useful for retrieving specific context from the MetaGPT paper.\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d2c152",
   "metadata": {},
   "source": [
    "## Define Router\n",
    "\n",
    "Llamaindex provides several diffent selectors.  \n",
    "\n",
    "- The LLM selectors use the LLM to output a JSON that is parsed, and corresponding indexes are queried.\n",
    "- The Pydantic selects use the OpenAI Function Calling API to produce pydantic selection objects rather than raw JSON.\n",
    "\n",
    "In this example we are using the LLMSingleSelector. The RouterQueryEngine takes in a selector type as well as a set of tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00734d7c-638a-4d63-ab1f-7f5a92a65119",
   "metadata": {
    "height": 217,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine.router_query_engine import RouterQueryEngine\n",
    "from llama_index.core.selectors import LLMSingleSelector\n",
    "\n",
    "query_engine = RouterQueryEngine(\n",
    "    selector=LLMSingleSelector.from_defaults(),\n",
    "    query_engine_tools=[\n",
    "        summary_tool,\n",
    "        vector_tool,\n",
    "    ],\n",
    "    # allows us to view the intermediate steps of the query engine\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe3f0a76-68a8-444d-867f-d084bb3ff112",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 0: This choice indicates that the document is useful for summarization questions related to MetaGPT..\n",
      "\u001b[0mThe document introduces MetaGPT, a meta-programming framework for multi-agent collaboration based on Large Language Models (LLMs). MetaGPT incorporates Standardized Operating Procedures (SOPs) to enhance the efficiency and accuracy of problem-solving processes by assigning specific roles to agents and streamlining workflows. It utilizes a communication protocol to enhance role communication efficiency and implements structured communication interfaces. MetaGPT achieves state-of-the-art performance in evaluations, demonstrating its robustness and efficiency in developing LLM-based multi-agent systems. Additionally, the document discusses the implementation of self-improvement mechanisms and multi-agent economies in software development using MetaGPT, exploring concepts like recursive self-improvement, self-referential mechanisms, dynamic collaboration processes, and the \"Economy of Minds.\" It also provides insights into the development process of a software application named \"Drawing App\" using MetaGPT, detailing steps from user input to the generation of a functional application. Furthermore, the document evaluates different GPT models, addresses limitations and ethical concerns related to MetaGPT, and highlights challenges and unique designs of the system.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What is the summary of the document?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3fedea0-f2a9-46bb-8aaf-287df65b8fff",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    }
   ],
   "source": [
    "print(len(response.source_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af8c31b3-8e22-4ad9-9825-b8de21bd03c0",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 1: Useful for retrieving specific context from the MetaGPT paper..\n",
      "\u001b[0mAgents share information with other agents by utilizing a shared message pool where they can publish structured messages and also subscribe to relevant messages based on their profiles. This shared message pool allows all agents to exchange messages directly, enabling them to access messages from other entities transparently. By storing information in this global message pool, agents can retrieve required information directly from the pool without the need to inquire about other agents and wait for their responses, thus enhancing communication efficiency.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"How do agents share information with other agents?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed060ee",
   "metadata": {},
   "source": [
    "## Let's put everything together\n",
    "\n",
    "Code from the above cells, have been put into the utils helper method.  Builds the query engine with both vector and summary search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8f92e0b-1c54-489b-b8dd-41ebaafb380a",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "from utils import get_router_query_engine\n",
    "\n",
    "query_engine = get_router_query_engine(\"/teamspace/uploads/metagpt.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec1a43f3-77dc-472a-8adc-56551c00a0ff",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 1: The ablation study results are specific context from the MetaGPT paper, making choice 2 the most relevant..\n",
      "\u001b[0mThe ablation study results indicate the effectiveness of MetaGPT in addressing challenges related to context efficiency, reducing hallucinations, and managing information overload in software development tasks. The study highlights how MetaGPT's unique designs successfully tackle issues such as eliminating ambiguity in natural language descriptions, maintaining information validity in lengthy contexts, reducing code hallucinations, and managing information overload through a global message pool and subscription mechanism.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"Tell me about the ablation study results?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37d8cfe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 1: The paper provides specific context from the MetaGPT paper, which may include coding examples..\n",
      "\u001b[0mYes, the paper provides specific coding examples in Table 8, which includes various software development tasks such as creating games, data processing programs, management programs, music transcriber, custom press releases, and interactive weather dashboard.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"Does the paper provide specific coding example?\")\n",
    "print(str(response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
