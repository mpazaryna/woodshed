{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd7c4c87",
   "metadata": {},
   "source": [
    "# Lesson 3: Multi Document Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357c97c9",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffc9b4f4-64d4-4266-9889-54db90e00ee9",
   "metadata": {
    "height": 64,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda8ddb9",
   "metadata": {},
   "source": [
    "## Setup an Agent Over Three Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "428d4e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
    "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
    "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
    "]\n",
    "\n",
    "papers = [\n",
    "    \"/teamspace/uploads/dlai/metagpt.pdf\",\n",
    "    \"/teamspace/uploads/dlai/longlora.pdf\",\n",
    "    \"/teamspace/uploads/dlai/selfrag.pdf\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7db8dea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper: /teamspace/uploads/dlai/metagpt.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper: /teamspace/uploads/dlai/longlora.pdf\n",
      "Getting tools for paper: /teamspace/uploads/dlai/selfrag.pdf\n"
     ]
    }
   ],
   "source": [
    "from utils import get_doc_tools\n",
    "from pathlib import Path\n",
    "\n",
    "paper_to_tools_dict = {}\n",
    "for paper in papers:\n",
    "    print(f\"Getting tools for paper: {paper}\")\n",
    "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
    "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d24f8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc0a1f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc3ef7eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(initial_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40931eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    initial_tools, \n",
    "    llm=llm, \n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db13c176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the evaluation dataset used in LongLoRA, and then tell me about the evaluation results\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"evaluation dataset\"}\n",
      "=== Function Output ===\n",
      "The evaluation dataset used in the research presented in the provided context includes the RedPajama dataset, PG19 dataset, LongBench dataset, and LEval dataset.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"evaluation results\"}\n",
      "=== Function Output ===\n",
      "The evaluation results presented in the provided context cover a range of aspects related to model performance, efficiency, and effectiveness across different tasks and context lengths. These results demonstrate the effectiveness of various methods such as S2-Attn and LoRA+ in achieving comparable performance to full fine-tuning while being more efficient. Additionally, the evaluation results highlight the impact of attention patterns on model performance during fine-tuning, with insights into the effectiveness of dilated attention and sparse attention in different scenarios. Furthermore, the evaluation results include comparisons of model performance on tasks like passkey retrieval accuracy, topic retrieval evaluation with LongChat, and long-sequence language modeling perplexity on datasets such as PG19 and the Arxiv Math proof-pile dataset. The models show varying levels of accuracy and performance based on the context lengths and tasks evaluated, with detailed analysis on the impact of context length on perplexity scores for models fine-tuned on RedPajama.\n",
      "=== LLM Response ===\n",
      "The evaluation dataset used in LongLoRA includes the RedPajama dataset, PG19 dataset, LongBench dataset, and LEval dataset. \n",
      "\n",
      "The evaluation results cover various aspects related to model performance, efficiency, and effectiveness across different tasks and context lengths. These results demonstrate the effectiveness of methods like S2-Attn and LoRA+ in achieving comparable performance to full fine-tuning while being more efficient. The impact of attention patterns on model performance during fine-tuning, such as dilated attention and sparse attention, is highlighted. The evaluation results also include comparisons of model performance on tasks like passkey retrieval accuracy, topic retrieval evaluation with LongChat, and long-sequence language modeling perplexity on datasets like PG19 and the Arxiv Math proof-pile dataset. The models show varying levels of accuracy and performance based on context lengths and tasks evaluated, with detailed analysis on the impact of context length on perplexity scores for models fine-tuned on RedPajama.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Tell me about the evaluation dataset used in LongLoRA, \"\n",
    "    \"and then tell me about the evaluation results\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0dd3a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Give me a summary of both Self-RAG and LongLoRA\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: summary_tool_selfrag with args: {\"input\": \"Self-RAG\"}\n",
      "=== Function Output ===\n",
      "Self-RAG is a framework that enhances the quality and factuality of a large language model through retrieval and self-reflection. It incorporates reflection tokens to evaluate its own output during both training and inference, consisting of a generator model, a retriever, and a critic model. By dynamically deciding when to retrieve text passages based on predictions and generating special tokens to evaluate its own predictions, Self-RAG aims to improve an LM's generation quality, including its factual accuracy, without compromising its versatility. This framework has shown significant performance advantages over supervised fine-tuned LLMs and existing retrieval-augmented models in various tasks, demonstrating substantial gains in improving factuality and citation accuracy for long-form generations.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"LongLoRA\"}\n",
      "=== Function Output ===\n",
      "LongLoRA is an efficient method for extending the context sizes of pre-trained large language models (LLMs) with minimal computational cost. It incorporates shifted sparse attention (S2-Attn) during training to enable context extension, resulting in significant computation savings while maintaining performance. LongLoRA combines this with an improved low-rank adaptation (LoRA) technique to allow for the extension of LLMs' context while preserving their original architectures. The approach has demonstrated strong empirical results on various tasks and models, showcasing its effectiveness in efficiently extending context lengths.\n",
      "=== LLM Response ===\n",
      "Here are summaries of Self-RAG and LongLoRA:\n",
      "\n",
      "1. Self-RAG:\n",
      "Self-RAG is a framework that enhances the quality and factuality of a large language model through retrieval and self-reflection. It incorporates reflection tokens to evaluate its own output during both training and inference, consisting of a generator model, a retriever, and a critic model. By dynamically deciding when to retrieve text passages based on predictions and generating special tokens to evaluate its own predictions, Self-RAG aims to improve an LM's generation quality, including its factual accuracy, without compromising its versatility. This framework has shown significant performance advantages over supervised fine-tuned LLMs and existing retrieval-augmented models in various tasks, demonstrating substantial gains in improving factuality and citation accuracy for long-form generations.\n",
      "\n",
      "2. LongLoRA:\n",
      "LongLoRA is an efficient method for extending the context sizes of pre-trained large language models (LLMs) with minimal computational cost. It incorporates shifted sparse attention (S2-Attn) during training to enable context extension, resulting in significant computation savings while maintaining performance. LongLoRA combines this with an improved low-rank adaptation (LoRA) technique to allow for the extension of LLMs' context while preserving their original architectures. The approach has demonstrated strong empirical results on various tasks and models, showcasing its effectiveness in efficiently extending context lengths.\n",
      "assistant: Here are summaries of Self-RAG and LongLoRA:\n",
      "\n",
      "1. Self-RAG:\n",
      "Self-RAG is a framework that enhances the quality and factuality of a large language model through retrieval and self-reflection. It incorporates reflection tokens to evaluate its own output during both training and inference, consisting of a generator model, a retriever, and a critic model. By dynamically deciding when to retrieve text passages based on predictions and generating special tokens to evaluate its own predictions, Self-RAG aims to improve an LM's generation quality, including its factual accuracy, without compromising its versatility. This framework has shown significant performance advantages over supervised fine-tuned LLMs and existing retrieval-augmented models in various tasks, demonstrating substantial gains in improving factuality and citation accuracy for long-form generations.\n",
      "\n",
      "2. LongLoRA:\n",
      "LongLoRA is an efficient method for extending the context sizes of pre-trained large language models (LLMs) with minimal computational cost. It incorporates shifted sparse attention (S2-Attn) during training to enable context extension, resulting in significant computation savings while maintaining performance. LongLoRA combines this with an improved low-rank adaptation (LoRA) technique to allow for the extension of LLMs' context while preserving their original architectures. The approach has demonstrated strong empirical results on various tasks and models, showcasing its effectiveness in efficiently extending context lengths.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\"Give me a summary of both Self-RAG and LongLoRA\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad0272",
   "metadata": {},
   "source": [
    "## Extend the Agent with Tool Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb7e1875",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9bb75c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an \"object\" index and retriever over these tools\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.objects import ObjectIndex\n",
    "\n",
    "obj_index = ObjectIndex.from_objects(\n",
    "    all_tools,\n",
    "    index_cls=VectorStoreIndex,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d195e6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_retriever = obj_index.as_retriever(similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3af52318",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = obj_retriever.retrieve(\n",
    "    \"Tell me about the eval dataset used in MetaGPT and SWE-Bench\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c2c0f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    tool_retriever=obj_retriever,\n",
    "    llm=llm, \n",
    "    system_prompt=\"\"\" \\\n",
    "You are an agent designed to answer queries over a set of given papers.\n",
    "Please always use the tools provided to answer a question. Do not rely on prior knowledge.\\\n",
    "\n",
    "\"\"\",\n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a2f422a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the evaluation dataset used in MetaGPT and compare it against SWE-Bench\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_metagpt with args: {\"input\": \"evaluation dataset used in MetaGPT\"}\n",
      "=== Function Output ===\n",
      "The evaluation dataset used in MetaGPT includes HumanEval, MBPP, and SoftwareDev.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_metagpt with args: {\"input\": \"SWE-Bench dataset\"}\n",
      "=== Function Output ===\n",
      "The SWE-Bench dataset is not referenced in the provided context information.\n",
      "=== LLM Response ===\n",
      "The evaluation dataset used in MetaGPT includes HumanEval, MBPP, and SoftwareDev. However, the SWE-Bench dataset is not referenced in the provided context information.\n",
      "assistant: The evaluation dataset used in MetaGPT includes HumanEval, MBPP, and SoftwareDev. However, the SWE-Bench dataset is not referenced in the provided context information.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Tell me about the evaluation dataset used \"\n",
    "    \"in MetaGPT and compare it against SWE-Bench\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c92a98a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Compare and contrast the LoRA papers (LongLoRA, LoftQ). Analyze the approach in each paper first. \n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"LoftQ\"}\n",
      "=== Function Output ===\n",
      "LoftQ is a method that focuses on efficient fine-tuning and retaining the original architecture during inference. It aims to save substantial fine-tuning costs while maintaining the quality of the original attention. LoftQ allows full access to the entire input via unmodified attention during inference.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"LongLoRA\"}\n",
      "=== Function Output ===\n",
      "LongLoRA is an efficient method for extending the context length of Large Language Models (LLMs) while minimizing additional trainable parameters and computational costs. It combines shifted sparse attention (S2-Attn) and LoRA to effectively fine-tune models with longer context windows, maintaining the original model architectures. This approach has demonstrated strong empirical results across various tasks, showing improved training speed and lower memory costs compared to full fine-tuning.\n",
      "=== LLM Response ===\n",
      "LoftQ focuses on efficient fine-tuning and retaining the original architecture during inference to save substantial costs while maintaining the quality of the original attention. It allows full access to the entire input via unmodified attention during inference.\n",
      "\n",
      "On the other hand, LongLoRA is an efficient method for extending the context length of Large Language Models (LLMs) while minimizing additional trainable parameters and computational costs. It combines shifted sparse attention (S2-Attn) and LoRA to effectively fine-tune models with longer context windows, maintaining the original model architectures. LongLoRA has shown strong empirical results across various tasks, demonstrating improved training speed and lower memory costs compared to full fine-tuning.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Compare and contrast the LoRA papers (LongLoRA, LoftQ). \"\n",
    "    \"Analyze the approach in each paper first. \"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
