{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain fleet-context openai pandas faiss-cpu # faiss-gpu for CUDA supported GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Env Variables\n",
    "\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = \"true\"\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = \"https://api.smith.langchain.com\"\n",
    "os.environ['LANGCHAIN_API_KEY'] = \"\"\n",
    "os.environ['LANGCHAIN_PROJECT'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from typing import Any, Optional, Type\n",
    "\n",
    "import pandas as pd\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.retrievers import MultiVectorRetriever\n",
    "from langchain.schema import Document\n",
    "from langchain.schema.storage import BaseStore\n",
    "from langchain.schema.vectorstore import VectorStore\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "\n",
    "def load_fleet_retriever(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    vectorstore_cls: Type[VectorStore] = FAISS,\n",
    "    docstore: Optional[BaseStore] = None,\n",
    "    **kwargs: Any,\n",
    "):\n",
    "    vectorstore = _populate_vectorstore(df, vectorstore_cls)\n",
    "    if docstore is None:\n",
    "        return vectorstore.as_retriever(**kwargs)\n",
    "    else:\n",
    "        _populate_docstore(df, docstore)\n",
    "        return MultiVectorRetriever(\n",
    "            vectorstore=vectorstore, docstore=docstore, id_key=\"parent\", **kwargs\n",
    "        )\n",
    "\n",
    "\n",
    "def _populate_vectorstore(\n",
    "    df: pd.DataFrame,\n",
    "    vectorstore_cls: Type[VectorStore],\n",
    ") -> VectorStore:\n",
    "    if not hasattr(vectorstore_cls, \"from_embeddings\"):\n",
    "        raise ValueError(\n",
    "            f\"Incompatible vector store class {vectorstore_cls}.\"\n",
    "            \"Must implement `from_embeddings` class method.\"\n",
    "        )\n",
    "    texts_embeddings = []\n",
    "    metadatas = []\n",
    "    for _, row in df.iterrows():\n",
    "        texts_embeddings.append((row.metadata[\"text\"], row[\"dense_embeddings\"]))\n",
    "        metadatas.append(row.metadata)\n",
    "    return vectorstore_cls.from_embeddings(\n",
    "        texts_embeddings,\n",
    "        OpenAIEmbeddings(model=\"text-embedding-ada-002\"),\n",
    "        metadatas=metadatas,\n",
    "    )\n",
    "\n",
    "\n",
    "def _populate_docstore(df: pd.DataFrame, docstore: BaseStore) -> None:\n",
    "    parent_docs = []\n",
    "    df = df.copy()\n",
    "    df[\"parent\"] = df.metadata.apply(itemgetter(\"parent\"))\n",
    "    for parent_id, group in df.groupby(\"parent\"):\n",
    "        sorted_group = group.iloc[\n",
    "            group.metadata.apply(itemgetter(\"section_index\")).argsort()\n",
    "        ]\n",
    "        text = \"\".join(sorted_group.metadata.apply(itemgetter(\"text\")))\n",
    "        metadata = {\n",
    "            k: sorted_group.iloc[0].metadata[k] for k in (\"title\", \"type\", \"url\")\n",
    "        }\n",
    "        text = metadata[\"title\"] + \"\\n\" + text\n",
    "        metadata[\"id\"] = parent_id\n",
    "        parent_docs.append(Document(page_content=text, metadata=metadata))\n",
    "    docstore.mset(((d.metadata[\"id\"], d) for d in parent_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from context import download_embeddings\n",
    "from langchain.storage import InMemoryStore\n",
    "df = download_embeddings(\"langchain\")\n",
    "vecstore_retriever = load_fleet_retriever(df,docstore=InMemoryStore(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecstore_retriever.get_relevant_documents(\"Using LCEL to create a tagging chain that passes output to one of three other extraction chains based on the tag it outputs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.storage import InMemoryStore\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are a great software engineer who is very familiar \\\n",
    "with Python. Given a user question or request about a new Python library called LangChain and \\\n",
    "parts of the LangChain documentation, answer the question or generate the requested code. \\\n",
    "Your answers must be accurate, should include code whenever possible, and should assume anything \\\n",
    "about LangChain which is note explicitly stated in the LangChain documentation. If the required \\\n",
    "information is not available, just say so.\n",
    "\n",
    "LangChain Documentation\n",
    "------------------\n",
    "\n",
    "{context}\"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo-16k\")\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"context\": vecstore_retriever\n",
    "        | (lambda docs: \"\\n\\n\".join(d.page_content for d in docs)),\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in chain.invoke(\"Using LCEL to create a tagging chain that passes output to one of three other extraction chains based on the tag it outputs.\"\n",
    "):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
